{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4 API: Extract Structured Data from Papers\n",
    "\n",
    "Before starting, create a `.env` file in the root directory of the project and add the following environment variables:\n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "```\n",
    "\n",
    "## Structure\n",
    "\n",
    "1. Create a new GPT assistant with PDF file search enabled.\n",
    "2. Create a list of file paths of our PDF papers saved in the `pdfs` directory.\n",
    "3. Use the assistant to extract structured data from a single PDF file.\n",
    "4. Standardize GPTs response to JSON format.\n",
    "5. Save the data to Airtable.\n",
    "6. [Exercise] Extract data from multiple PDFs in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a new GPT Assistant with File Search Enabled\n",
    "\n",
    "The Assistants API allows you to build AI assistants within your own applications. An Assistant has instructions and can leverage models, tools, and files to respond to user queries. The Assistants API currently supports three types of tools: Code Interpreter, File Search, and Function calling.\n",
    "\n",
    "- [GPT Assistants API documentation](https://platform.openai.com/docs/assistants/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install python-dotenv openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load OPENAI_API_KEY value from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant 'Paper Data Extractor' found with ID: asst_WQsHaxQTTCkAPUbn6nXS1kbr\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "assistant_name = \"Paper Data Extractor\"\n",
    "\n",
    "# List available assistants and create a new one if \"Paper Data Extractor\" is not in the list\n",
    "assistants = client.beta.assistants.list()\n",
    "assistant_names = [assistant.name for assistant in assistants.data]\n",
    "\n",
    "# If assistant with the given name does not exist, create a new one\n",
    "if assistant_name not in assistant_names:\n",
    "    assistant = client.beta.assistants.create(\n",
    "        name=assistant_name,\n",
    "        model=\"gpt-4-turbo\",\n",
    "        tools=[{\"type\": \"file_search\"}],\n",
    "    )\n",
    "    print(f\"New assistant created: {assistant.id}\")\n",
    "# If assistant with the given name exists, use it\n",
    "else:\n",
    "    assistant = assistants.data[assistant_names.index(assistant_name)]\n",
    "    print(f\"Assistant '{assistant_name}' found with ID: {assistant.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read the PDF file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pdfs/1-Tracking Real Time Layoffs with SEC Filings - A Preliminary Investigation.pdf',\n",
       " 'pdfs/2-Overnight Post-Earnings Announcement Drift and SEC Form 8-K Disclosures.pdf',\n",
       " 'pdfs/3-Forecasting Stock Excess Returns With SEC 8-K Filings.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all files in \"current dir/pdfs\" and save to file_paths\n",
    "import os\n",
    "\n",
    "file_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(\"pdfs\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use assistant to extract data from a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 'pdfs/1-Tracking Real Time Layoffs with SEC Filings - A Preliminary Investigation.pdf' to OpenAI\n"
     ]
    }
   ],
   "source": [
    "# Upload a single PDF to the assistant\n",
    "file_path = file_paths[0]\n",
    "message_file = client.files.create(file=open(file_path, \"rb\"), purpose=\"assistants\")\n",
    "print(f\"Uploaded '{file_path}' to OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolResourcesFileSearch(vector_store_ids=['vs_PacY8x8TzTdslL6ST0p94Q7K'])\n"
     ]
    }
   ],
   "source": [
    "# Here's where the magic happens!\n",
    "# We ask GPT to extract data from the PDF and return it as a structured JSON object\n",
    "prompt = \"\"\"Extract the following data from the provided paper: Title, the research questions, the types of data used for the study, the size of the data set (i.e. how many samples were analyzed), the history of the dataset (i.e. how many years does it cover), the source of the data, the methods used to answer the research questions, the various metrics used for measuring, and the outcomes the authors found. Return the extracted structured data as a JSON object. Only respond with the JSON object, and do not respond with anything else.\n",
    "\n",
    "Return your response as a structured JSON object using the following format:\n",
    "''' \n",
    "{\n",
    "  \"title_of_paper\": \"What is the title of the paper?\", // string: the title of the paper\n",
    "  \"research_questions\": [\"What is the research question?\"], // array of strings: the research questions. If there are multiple research questions, list them all as separate items in the array\n",
    "  \"data_types\": [\"What types of data were used?\"], // array of strings: the types of data used for the study. If there are multiple types of data, list them all as separate items in the array\n",
    "  \"data_size\": \"What is the size of the dataset?\", // string: the size of the data set, i.e. number of observations, samples, etc.\n",
    "  \"data_history\": \"How many years does the dataset cover?\", // string: the history of the dataset\n",
    "  \"data_sources\": [\"What are the sources of the data?\"], // array of string: the sources of the data. If there are multiple sources, list them all as separate items in the array\n",
    "  \"methods\": [\"What methods were used to answer the research questions?\"], // array of string: the methods used to answer the research questions. If there are multiple methods, list them all as separate items in the array\n",
    "  \"metrics\": [\"What metrics were used for measuring?\"], // array of string: the various metrics used for measuring. If there are multiple metrics, list them all as separate items in the array\n",
    "  \"outcomes\": [\"What outcomes did the authors find?\"] // array of string: the outcomes the authors found. If there are multiple outcomes, list them all as separate items in the array\n",
    "}\n",
    "'''\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "# Create a conversation thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "            \"attachments\": [\n",
    "                {\"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}]}\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"title_of_paper\": \"Tracking Real Time Layoffs with SEC Filings: A Preliminary Investigation\",\n",
      "  \"research_questions\": [\"How are 8-K filings used as a new source of data on layoffs?\", \"How do 8-K filed layoffs correlate with other unemployment and layoff indicators?\", \"How can 8-K filings be used to forecast important labor market dynamics?\"],\n",
      "  \"data_types\": [\"Company names from 8-K filings\", \"Linked WARN notices\", \"Employment data from Compustat\"],\n",
      "  \"data_size\": \"285 linked layoffs\",\n",
      "  \"data_history\": \"Covers recent years including specific references to 2022 and 2023\",\n",
      "  \"data_sources\": [\"8-K filings\", \"WARN Notices\", \"Compustat employment data\"],\n",
      "  \"methods\": [\"Sentence embeddings using BERT\", \"Large language models (Llama 2) for natural language processing\", \"Analyzing filing dates and employment data correlation\", \"Fuzzy matching of company names between WARN and 8-K data\"],\n",
      "  \"metrics\": [\"Number of reported layoff events\", \"Number of affected workers\", \"Time difference in reporting between WARN notices and 8-K filings\", \"Correlation of layoffs data with business cycle and other layoffs indicators\"],\n",
      "  \"outcomes\": [\"8-K filings provide timely data on layoffs\", \"8-K filings data does not cover all layoffs but captures significant subset\", \"8-K filings data can be useful for forecasting unemployment rate and initial unemployment insurance claims\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Request the assistant to run the thread and create a response\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "# Remove annotations from response\n",
    "for index, annotation in enumerate(message_content.annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, \"\")\n",
    "\n",
    "print(message_content.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Convert GPT's string output to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title_of_paper': 'Tracking Real Time Layoffs with SEC Filings: A Preliminary Investigation',\n",
       " 'research_questions': ['How are 8-K filings used as a new source of data on layoffs?',\n",
       "  'How do 8-K filed layoffs correlate with other unemployment and layoff indicators?',\n",
       "  'How can 8-K filings be used to forecast important labor market dynamics?'],\n",
       " 'data_types': ['Company names from 8-K filings',\n",
       "  'Linked WARN notices',\n",
       "  'Employment data from Compustat'],\n",
       " 'data_size': '285 linked layoffs',\n",
       " 'data_history': 'Covers recent years including specific references to 2022 and 2023',\n",
       " 'data_sources': ['8-K filings', 'WARN Notices', 'Compustat employment data'],\n",
       " 'methods': ['Sentence embeddings using BERT',\n",
       "  'Large language models (Llama 2) for natural language processing',\n",
       "  'Analyzing filing dates and employment data correlation',\n",
       "  'Fuzzy matching of company names between WARN and 8-K data'],\n",
       " 'metrics': ['Number of reported layoff events',\n",
       "  'Number of affected workers',\n",
       "  'Time difference in reporting between WARN notices and 8-K filings',\n",
       "  'Correlation of layoffs data with business cycle and other layoffs indicators'],\n",
       " 'outcomes': ['8-K filings provide timely data on layoffs',\n",
       "  '8-K filings data does not cover all layoffs but captures significant subset',\n",
       "  '8-K filings data can be useful for forecasting unemployment rate and initial unemployment insurance claims']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Remove starting \"```json\" and ending \"```\" values from GPTs response\n",
    "json_string = (\n",
    "    message_content.value.replace(\"```json\", \"\")\n",
    "    .replace(\"```\", \"\")\n",
    "    .replace(\"\\n\", \"\")  # Remove newlines\n",
    ")\n",
    "# Remove annotations from JSON string\n",
    "json_string = re.sub(r\"【.*】\", \"\", json_string)\n",
    "# Convert message_content.value to JSON\n",
    "extracted_data_json = json.loads(json_string)\n",
    "\n",
    "extracted_data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Tracking Real Time Layoffs with SEC Filings: A Preliminary Investigation\n",
      "Research Questions:\n",
      " - How are 8-K filings used as a new source of data on layoffs?\n",
      " - How do 8-K filed layoffs correlate with other unemployment and layoff indicators?\n",
      " - How can 8-K filings be used to forecast important labor market dynamics?\n",
      "Outcomes:\n",
      " - 8-K filings provide timely data on layoffs\n",
      " - 8-K filings data does not cover all layoffs but captures significant subset\n",
      " - 8-K filings data can be useful for forecasting unemployment rate and initial unemployment insurance claims\n"
     ]
    }
   ],
   "source": [
    "print(\"Title:\", extracted_data_json[\"title_of_paper\"])\n",
    "print(\"Research Questions:\\n - \" + \"\\n - \".join(extracted_data_json[\"research_questions\"]))\n",
    "print(\"Outcomes:\\n - \" + \"\\n - \".join(extracted_data_json[\"outcomes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_of_paper</th>\n",
       "      <th>research_questions</th>\n",
       "      <th>data_types</th>\n",
       "      <th>data_size</th>\n",
       "      <th>data_history</th>\n",
       "      <th>data_sources</th>\n",
       "      <th>methods</th>\n",
       "      <th>metrics</th>\n",
       "      <th>outcomes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tracking Real Time Layoffs with SEC Filings: A...</td>\n",
       "      <td>[How are 8-K filings used as a new source of d...</td>\n",
       "      <td>[Company names from 8-K filings, Linked WARN n...</td>\n",
       "      <td>285 linked layoffs</td>\n",
       "      <td>Covers recent years including specific referen...</td>\n",
       "      <td>[8-K filings, WARN Notices, Compustat employme...</td>\n",
       "      <td>[Sentence embeddings using BERT, Large languag...</td>\n",
       "      <td>[Number of reported layoff events, Number of a...</td>\n",
       "      <td>[8-K filings provide timely data on layoffs, 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title_of_paper  \\\n",
       "0  Tracking Real Time Layoffs with SEC Filings: A...   \n",
       "\n",
       "                                  research_questions  \\\n",
       "0  [How are 8-K filings used as a new source of d...   \n",
       "\n",
       "                                          data_types           data_size  \\\n",
       "0  [Company names from 8-K filings, Linked WARN n...  285 linked layoffs   \n",
       "\n",
       "                                        data_history  \\\n",
       "0  Covers recent years including specific referen...   \n",
       "\n",
       "                                        data_sources  \\\n",
       "0  [8-K filings, WARN Notices, Compustat employme...   \n",
       "\n",
       "                                             methods  \\\n",
       "0  [Sentence embeddings using BERT, Large languag...   \n",
       "\n",
       "                                             metrics  \\\n",
       "0  [Number of reported layoff events, Number of a...   \n",
       "\n",
       "                                            outcomes  \n",
       "0  [8-K filings provide timely data on layoffs, 8...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the JSON response to a pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "extracted_data_df = pd.DataFrame([extracted_data_json])\n",
    "\n",
    "extracted_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Store extracted data to Airtable\n",
    "\n",
    "Airtable looks like a spreadsheet but acts like a database. It's a platform that allows you to organize and structure data. In order to add data to Airtable, you need to create a new base and table, and then either use front-end/website or API webhooks to add records to the table. In this case, we will use the API webhook to add records to the table.\n",
    "\n",
    "- Create a new base and table in [Airtable](https://airtable.com/).\n",
    "- Create a new automation using the \"When webhook received\" trigger and \"Create record\" action.\n",
    "- Map the JSON fields to the fields in the Airtable table inside the \"Create record\" tab. For example, map the `title_of_paper` field to the `Title` field in the Airtable table.\n",
    "\n",
    "Alternative storage options: Python Pandas dataframe saved to local Parquet file, MongoDB, SQL databases (MySQL, PostgreSQL, etc), DynamoDB, Google Sheets, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The webhook URL for the table in Airtable\n",
    "webhook_url = \"https://hooks.airtable.com/workflows/v1/genericWebhook/apphgC5p4...\"\n",
    "\n",
    "# Make the POST request to insert the data\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "response = requests.post(webhook_url, json=extracted_data_json, headers=headers)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Data inserted successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to insert data: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airtable table result after processing the first two PDFs:\n",
    "\n",
    "![Airtable Table](https://i.imgur.com/0KCixZn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Process all PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Develop methods to extract structured data from multiple PDFs \n",
    "# and insert them into the Airtable table.\n",
    "# Bonus: Use `pandarallel` to parallelize the extraction process and speed up the data extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
