{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4 API: Extract Structured Data from PDFs\n",
    "\n",
    "Before starting, create a `.env` file in the root directory of the project and add the following environment variables:\n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a new Assistant with File Search Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OPENAI_API_KEY value from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant created: asst_WQsHaxQTTCkAPUbn6nXS1kbr\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "assistant_name = \"Paper Data Extractor\"\n",
    "\n",
    "# List available assistants and create a new one if \"Paper Data Extractor\" is not in the list\n",
    "assistants = client.beta.assistants.list()\n",
    "assistant_names = [assistant.name for assistant in assistants.data]\n",
    "\n",
    "if assistant_name not in assistant_names:\n",
    "    assistant = client.beta.assistants.create(\n",
    "        name=assistant_name,\n",
    "        model=\"gpt-4-turbo\",\n",
    "        tools=[{\"type\": \"file_search\"}],\n",
    "    )\n",
    "    print(f\"New assistant created: {assistant.id}\")\n",
    "else:\n",
    "    assistant = assistants.data[assistant_names.index(assistant_name)]\n",
    "    print(f\"Assistant '{assistant_name}' found with ID: {assistant.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read the PDF file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pdfs/1-Tracking Real Time Layoffs with SEC Filings - A Preliminary Investigation.pdf',\n",
       " 'pdfs/2-Overnight Post-Earnings Announcement Drift and SEC Form 8-K Disclosures.pdf',\n",
       " 'pdfs/3-Forecasting Stock Excess Returns With SEC 8-K Filings.pdf']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all files in \"current dir/pdfs\" and save to file_paths \n",
    "import os\n",
    "file_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(\"pdfs\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract the relevant information from a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 'pdfs/1-Tracking Real Time Layoffs with SEC Filings - A Preliminary Investigation.pdf' to OpenAI\n"
     ]
    }
   ],
   "source": [
    "# Upload a single PDF to the assistant\n",
    "file_path = file_paths[0]\n",
    "message_file = client.files.create(\n",
    "  file=open(file_path, \"rb\"), \n",
    "  purpose=\"assistants\"\n",
    ")\n",
    "print(f\"Uploaded '{file_path}' to OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolResourcesFileSearch(vector_store_ids=['vs_IueNY5vx0uHzxkc89uVUCiS4'])\n"
     ]
    }
   ],
   "source": [
    "# Extract the structured data from the PDF\n",
    "prompt = \"\"\"Extract the following data from the provided paper: Title, the research questions, the types of data used for the study, the size of the data set, the history of the dataset (i.e. how many years does it cover), the source of the data, the methods used to answer the research questions, the various metrics used for measuring, and the outcomes the authors found. Return the extracted structured data as a JSON object. Only respond with the JSON object, and do not respond with anything else.\n",
    "\n",
    "Return your response as a structured JSON object using the following format:\n",
    "''' \n",
    "{\n",
    "  \"title_of_paper\": \"What is the title of the paper?\", // string: the title of the paper\n",
    "  \"research_questions\": [\"What is the research question?\"], // array of strings: the research questions. If there are multiple research questions, list them all as separate items in the array\n",
    "  \"data_types\": [\"What types of data were used?\"], // array of strings: the types of data used for the study. If there are multiple types of data, list them all as separate items in the array\n",
    "  \"data_size\": \"What is the size of the dataset?\", // string: the size of the data set, i.e. number of observations, samples, etc.\n",
    "  \"data_history\": \"How many years does the dataset cover?\", // string: the history of the dataset\n",
    "  \"data_sources\": [\"What are the sources of the data?\"], // array of string: the sources of the data. If there are multiple sources, list them all as separate items in the array\n",
    "  \"methods\": [\"What methods were used to answer the research questions?\"], // array of string: the methods used to answer the research questions. If there are multiple methods, list them all as separate items in the array\n",
    "  \"metrics\": [\"What metrics were used for measuring?\"], // array of string: the various metrics used for measuring. If there are multiple metrics, list them all as separate items in the array\n",
    "  \"outcomes\": [\"What outcomes did the authors find?\"] // array of string: the outcomes the authors found. If there are multiple outcomes, list them all as separate items in the array\n",
    "}\n",
    "'''\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "# Create a conversation thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "            \"attachments\": [\n",
    "                {\"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}]}\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"title_of_paper\": \"Tracking Real Time Layoffs with SEC Filings: A Preliminary Investigation\",\n",
      "  \"research_questions\": [\n",
      "    \"How are layoff indicators from 8-K filings correlated with the business cycle?\",\n",
      "    \"How does the industry composition differ in 8-K filings?\",\n",
      "    \"Are 8-K filings preemptive in relation to WARN notices for layoffs?\"\n",
      "  ],\n",
      "  \"data_types\": [\"Company layoff announcements\", \"SEC filings data\", \"WARN notices\"],\n",
      "  \"data_size\": \"Over 7,000 layoffs identified, with detailed specifics provided for those linked to 8-K filings.\",\n",
      "  \"data_history\": \"The dataset covers historical layoffs including during the Great Recession and the Covid pandemic.\",\n",
      "  \"data_sources\": [\"SEC filings\", \"WARN notices\", \"Compustat employment data\"],\n",
      "  \"methods\": [\n",
      "    \"Natural Language Processing with BERT and Llama 2 models.\",\n",
      "    \"Regression analysis with VAR model specification.\",\n",
      "    \"Link-analysis between 8-K filings and WARN notices\"\n",
      "  ],\n",
      "  \"metrics\": [\n",
      "    \"Number of layoffs\",\n",
      "    \"Number of companies affected\",\n",
      "    \"Percentage changes in layoffs relative to previous years\",\n",
      "    \"Comparison of forecasts with actual unemployment rates\"\n",
      "  ],\n",
      "  \"outcomes\": [\n",
      "    \"Confirmed predictive power of the 8-K filings data for labor market indicators.\",\n",
      "    \"Different industrial impact seen in 8-K filings, especially manufacturing.\",\n",
      "    \"Timeliness comparison between 8-K filings and WARN notices showing mixed results.\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Request the assistant to run the thread and create a response\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "# Remove annotations from response\n",
    "for index, annotation in enumerate(message_content.annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, \"\")\n",
    "\n",
    "print(message_content.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Convert GPT's output to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title_of_paper': 'Tracking Real Time Layoffs with SEC Filings: A Preliminary Investigation',\n",
       " 'research_questions': ['How are layoff indicators from 8-K filings correlated with the business cycle?',\n",
       "  'How does the industry composition differ in 8-K filings?',\n",
       "  'Are 8-K filings preemptive in relation to WARN notices for layoffs?'],\n",
       " 'data_types': ['Company layoff announcements',\n",
       "  'SEC filings data',\n",
       "  'WARN notices'],\n",
       " 'data_size': 'Over 7,000 layoffs identified, with detailed specifics provided for those linked to 8-K filings.',\n",
       " 'data_history': 'The dataset covers historical layoffs including during the Great Recession and the Covid pandemic.',\n",
       " 'data_sources': ['SEC filings', 'WARN notices', 'Compustat employment data'],\n",
       " 'methods': ['Natural Language Processing with BERT and Llama 2 models.',\n",
       "  'Regression analysis with VAR model specification.',\n",
       "  'Link-analysis between 8-K filings and WARN notices'],\n",
       " 'metrics': ['Number of layoffs',\n",
       "  'Number of companies affected',\n",
       "  'Percentage changes in layoffs relative to previous years',\n",
       "  'Comparison of forecasts with actual unemployment rates'],\n",
       " 'outcomes': ['Confirmed predictive power of the 8-K filings data for labor market indicators.',\n",
       "  'Different industrial impact seen in 8-K filings, especially manufacturing.',\n",
       "  'Timeliness comparison between 8-K filings and WARN notices showing mixed results.']}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Remove starting \"```json\" and ending \"```\" values from GPTs response\n",
    "json_string = (\n",
    "    message_content.value.replace(\"```json\\n\", \"\").replace(\"```\", \"\").replace(\"\\n\", \"\")\n",
    ")\n",
    "# Remove annotations from JSON string\n",
    "json_string = re.sub(r\"【.*】\", \"\", json_string)\n",
    "# Convert message_content.value to JSON\n",
    "json_response = json.loads(json_string)\n",
    "\n",
    "json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_of_paper</th>\n",
       "      <th>research_questions</th>\n",
       "      <th>data_types</th>\n",
       "      <th>data_size</th>\n",
       "      <th>data_history</th>\n",
       "      <th>data_sources</th>\n",
       "      <th>methods</th>\n",
       "      <th>metrics</th>\n",
       "      <th>outcomes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tracking Real Time Layoffs with SEC Filings: A...</td>\n",
       "      <td>[How are layoff indicators from 8-K filings co...</td>\n",
       "      <td>[Company layoff announcements, SEC filings dat...</td>\n",
       "      <td>Over 7,000 layoffs identified, with detailed s...</td>\n",
       "      <td>The dataset covers historical layoffs includin...</td>\n",
       "      <td>[SEC filings, WARN notices, Compustat employme...</td>\n",
       "      <td>[Natural Language Processing with BERT and Lla...</td>\n",
       "      <td>[Number of layoffs, Number of companies affect...</td>\n",
       "      <td>[Confirmed predictive power of the 8-K filings...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title_of_paper  \\\n",
       "0  Tracking Real Time Layoffs with SEC Filings: A...   \n",
       "\n",
       "                                  research_questions  \\\n",
       "0  [How are layoff indicators from 8-K filings co...   \n",
       "\n",
       "                                          data_types  \\\n",
       "0  [Company layoff announcements, SEC filings dat...   \n",
       "\n",
       "                                           data_size  \\\n",
       "0  Over 7,000 layoffs identified, with detailed s...   \n",
       "\n",
       "                                        data_history  \\\n",
       "0  The dataset covers historical layoffs includin...   \n",
       "\n",
       "                                        data_sources  \\\n",
       "0  [SEC filings, WARN notices, Compustat employme...   \n",
       "\n",
       "                                             methods  \\\n",
       "0  [Natural Language Processing with BERT and Lla...   \n",
       "\n",
       "                                             metrics  \\\n",
       "0  [Number of layoffs, Number of companies affect...   \n",
       "\n",
       "                                            outcomes  \n",
       "0  [Confirmed predictive power of the 8-K filings...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the JSON response to a pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "json_response_df = pd.DataFrame([json_response])\n",
    "\n",
    "json_response_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Store extracted data to Airtable\n",
    "\n",
    "- Create a new base and table in [Airtable](https://airtable.com/)\n",
    "- Create a new automation using the \"When webhook received\" trigger and \"Create record\" action\n",
    "- Map the JSON fields to the fields in the Airtable table inside the \"Create record\" tab. For example, map the `title_of_paper` field to the `Title` field in the Airtable table.\n",
    "\n",
    "Alternative storage options: Python Pandas dataframe saved to local Parquet file, MongoDB, SQL databases (MySQL, PostgreSQL, etc), DynamoDB, Google Sheets, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The webhook URL for the table in Airtable\n",
    "webhook_url = \"https://hooks.airtable.com/workflows/v1/genericWebhook/apphgC5p4Jgz2VV5w/wflPQAUElCuobq4wO/wtrRmZljovvAMXQYJ\"\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Make the POST request to insert the data\n",
    "response = requests.post(webhook_url, json=json_response, headers=headers)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Data inserted successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to insert data: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airtable table result after processing the first two PDFs:\n",
    "\n",
    "![Airtable Table](assets/Airtable-table.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Process all PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Develop methods to extract structured data from multiple PDFs \n",
    "# and insert them into the Airtable table.\n",
    "# Bonus: Use `pandarallel` to parallelize the extraction process and speed up the data extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
