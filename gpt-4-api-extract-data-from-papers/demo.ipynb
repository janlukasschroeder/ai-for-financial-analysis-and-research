{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load OPENAI_API_KEY from .env file\n",
    "# !pip -q install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a new Assistant with File Search Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI()\n",
    " \n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Paper Data Extractor v1.1\",\n",
    "  model=\"gpt-4-turbo\",\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read the PDF file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pdfs/Tracking Real Time Layoffs with SEC Filings - A Preliminary Investigation.pdf']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all files in \"current dir/pdfs\" and save to file_paths \n",
    "import os\n",
    "file_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(\"pdfs\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract the relevant information from a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolResourcesFileSearch(vector_store_ids=['vs_PpGJ2dbvfZ8Wv809tyi23gh0'])\n"
     ]
    }
   ],
   "source": [
    "# Upload a single PDF to the assistant\n",
    "message_file = client.files.create(\n",
    "  file=open(file_paths[0], \"rb\"), \n",
    "  purpose=\"assistants\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolResourcesFileSearch(vector_store_ids=['vs_gvmYTO1VpvkKVMumrei2Gsl4'])\n"
     ]
    }
   ],
   "source": [
    "# Extract the structured data from the PDF\n",
    "prompt = \"\"\"Extract the following data from the provided paper: Title, the research questions, the types of data used for the study, the size of the data set, the history of the dataset (i.e. how many years does it cover), the source of the data, the methods used to answer the research questions, the various metrics used for measuring, and the outcomes the authors found. Return the extracted structured data as a JSON object. Only respond with the JSON object, and do not respond with anything else.\n",
    "\n",
    "Return your response as a structured JSON object using the following format:\n",
    "''' \n",
    "{\n",
    "  \"title_of_paper\": \"What is the title of the paper?\", // string: the title of the paper\n",
    "  \"research_questions\": [\"What is the research question?\"], // array of strings: the research questions. If there are multiple research questions, list them all as separate items in the array\n",
    "  \"data_types\": [\"What types of data were used?\"], // array of strings: the types of data used for the study. If there are multiple types of data, list them all as separate items in the array\n",
    "  \"data_size\": \"What is the size of the dataset?\", // string: the size of the data set, i.e. number of observations, samples, etc.\n",
    "  \"data_history\": \"How many years does the dataset cover?\", // string: the history of the dataset\n",
    "  \"data_sources\": [\"What are the sources of the data?\"], // array of string: the sources of the data. If there are multiple sources, list them all as separate items in the array\n",
    "  \"methods\": [\"What methods were used to answer the research questions?\"], // array of string: the methods used to answer the research questions. If there are multiple methods, list them all as separate items in the array\n",
    "  \"metrics\": [\"What metrics were used for measuring?\"], // array of string: the various metrics used for measuring. If there are multiple metrics, list them all as separate items in the array\n",
    "  \"outcomes\": [\"What outcomes did the authors find?\"] // array of string: the outcomes the authors found. If there are multiple outcomes, list them all as separate items in the array\n",
    "}\n",
    "'''\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "# Create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "            \"attachments\": [\n",
    "                {\"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}]}\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"title_of_paper\": \"Tracking Real Time Layoffs with SEC Filings: A Preliminary Investigation\",\n",
      "  \"research_questions\": [\n",
      "    \"What are the alternative timely indicators of layoffs based on 8-K filings?\"\n",
      "  ],\n",
      "  \"data_types\": [\n",
      "    \"8-K filings data\",\n",
      "    \"WARN notices data\",\n",
      "    \"firm-level data\"\n",
      "  ],\n",
      "  \"data_size\": \"285 linked layoffs between WARN notices and 8-K filings\",\n",
      "  \"data_history\": \"Covers multiple years, includes two recessions\",\n",
      "  \"data_sources\": [\n",
      "    \"8-K filings\",\n",
      "    \"WARN notices\",\n",
      "    \"Compustat\"\n",
      "  ],\n",
      "  \"methods\": [\n",
      "    \"Natural language processing\",\n",
      "    \"Sentence embeddings from BERT\",\n",
      "    \"Prompting generative large language model (Llama 2)\",\n",
      "    \"Quantitative regression analysis\"\n",
      "  ],\n",
      "  \"metrics\": [\n",
      "    \"Number of reported layoff events\",\n",
      "    \"Number of affected workers\"\n",
      "  ],\n",
      "  \"outcomes\": [\n",
      "    \"8-K filings are sometimes available before WARN notices\",\n",
      "    \"The 8-K layoff series are highly correlated with the business cycle and other layoff indicators\",\n",
      "    \"Preliminary evidence that the 8-K series are useful for forecasting important quantities such as the unemployment rate and initial unemployment insurance claims\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Create a run and poll the status of the run until it's in a terminal state\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "# Remove annotations from response\n",
    "for index, annotation in enumerate(message_content.annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, \"\")\n",
    "\n",
    "print(message_content.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Convert GPT's output to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title_of_paper': 'Tracking Real Time Layoffs with SEC Filings: A Preliminary Investigation',\n",
       " 'research_questions': ['What are the alternative timely indicators of layoffs based on 8-K filings?'],\n",
       " 'data_types': ['8-K filings data', 'WARN notices data', 'firm-level data'],\n",
       " 'data_size': '285 linked layoffs between WARN notices and 8-K filings',\n",
       " 'data_history': 'Covers multiple years, includes two recessions',\n",
       " 'data_sources': ['8-K filings', 'WARN notices', 'Compustat'],\n",
       " 'methods': ['Natural language processing',\n",
       "  'Sentence embeddings from BERT',\n",
       "  'Prompting generative large language model (Llama 2)',\n",
       "  'Quantitative regression analysis'],\n",
       " 'metrics': ['Number of reported layoff events', 'Number of affected workers'],\n",
       " 'outcomes': ['8-K filings are sometimes available before WARN notices',\n",
       "  'The 8-K layoff series are highly correlated with the business cycle and other layoff indicators',\n",
       "  'Preliminary evidence that the 8-K series are useful for forecasting important quantities such as the unemployment rate and initial unemployment insurance claims']}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Remove starting \"```json\" and ending \"```\" values from GPTs response\n",
    "json_string = (\n",
    "    message_content.value.replace(\"```json\\n\", \"\").replace(\"```\", \"\").replace(\"\\n\", \"\")\n",
    ")\n",
    "# Remove annotations from JSON string\n",
    "json_string = re.sub(r\"【.*】\", \"\", json_string)\n",
    "# Convert message_content.value to JSON\n",
    "json_response = json.loads(json_string)\n",
    "\n",
    "json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_of_paper</th>\n",
       "      <th>research_questions</th>\n",
       "      <th>data_types</th>\n",
       "      <th>data_size</th>\n",
       "      <th>data_history</th>\n",
       "      <th>data_sources</th>\n",
       "      <th>methods</th>\n",
       "      <th>metrics</th>\n",
       "      <th>outcomes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tracking Real Time Layoffs with SEC Filings: A...</td>\n",
       "      <td>[What are the alternative timely indicators of...</td>\n",
       "      <td>[8-K filings data, WARN notices data, firm-lev...</td>\n",
       "      <td>285 linked layoffs between WARN notices and 8-...</td>\n",
       "      <td>Covers multiple years, includes two recessions</td>\n",
       "      <td>[8-K filings, WARN notices, Compustat]</td>\n",
       "      <td>[Natural language processing, Sentence embeddi...</td>\n",
       "      <td>[Number of reported layoff events, Number of a...</td>\n",
       "      <td>[8-K filings are sometimes available before WA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title_of_paper  \\\n",
       "0  Tracking Real Time Layoffs with SEC Filings: A...   \n",
       "\n",
       "                                  research_questions  \\\n",
       "0  [What are the alternative timely indicators of...   \n",
       "\n",
       "                                          data_types  \\\n",
       "0  [8-K filings data, WARN notices data, firm-lev...   \n",
       "\n",
       "                                           data_size  \\\n",
       "0  285 linked layoffs between WARN notices and 8-...   \n",
       "\n",
       "                                     data_history  \\\n",
       "0  Covers multiple years, includes two recessions   \n",
       "\n",
       "                             data_sources  \\\n",
       "0  [8-K filings, WARN notices, Compustat]   \n",
       "\n",
       "                                             methods  \\\n",
       "0  [Natural language processing, Sentence embeddi...   \n",
       "\n",
       "                                             metrics  \\\n",
       "0  [Number of reported layoff events, Number of a...   \n",
       "\n",
       "                                            outcomes  \n",
       "0  [8-K filings are sometimes available before WA...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the JSON response to a pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "json_response_df = pd.DataFrame([json_response])\n",
    "\n",
    "json_response_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Store extracted data to Airtable\n",
    "\n",
    "- Create a new table in Airtable\n",
    "- Create a new automation using the \"When webhook received\" trigger and \"Create record\" action\n",
    "- Map the JSON fields to the fields in the Airtable table inside the \"Create record\" tab. For example, map the `title_of_paper` field to the `Title` field in the Airtable table.\n",
    "\n",
    "Alternative storage options: Python Pandas dataframe saved to local Parquet file, MongoDB, SQL databases (MySQL, PostgreSQL, etc), DynamoDB, Google Sheets, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The webhook URL for the table in Airtable\n",
    "webhook_url = \"https://hooks.airtable.com/workflows/v1/genericWebhook/apphgC5p4Jgz2VV5w/wflPQAUElCuobq4wO/wtrRmZljovvAMXQYJ\"\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Make the POST request to insert the data\n",
    "response = requests.post(webhook_url, json=json_response, headers=headers)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Data inserted successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to insert data: {response.text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
